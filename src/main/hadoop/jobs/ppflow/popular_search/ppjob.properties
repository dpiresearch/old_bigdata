# The hdfs directory where logs will be stored
hdfs_ingest_dir=/user/hadoop/data/pp/ingest
#hdfs_ingest_dir=/user/hadoop/data/ingest

# The tomcat directory where logs will be pulled from
# TODO: remove when we remove ingest from the workflow
#tomcat_dir=/var/log/tomcat7multi/frontend
tomcat_dir=/home/hadoop/tmp

# Directory to stage files before hdfs ingestion
# TODO: remove when we remove ingest from the workflow
stg_dir=/home/hadoop/data/stg

# Number of web hosts
# TODO: remove when we remove ingest from the workflow
num_web_hosts=2
web_host_base_name=web

# Input directory for the intermediate jobs
inputdir=/user/hadoop/data/pp/jobs

# The output directory for intermediate and final results
outputdir=/user/hadoop/data/pp/jobs

# The deployment directory
deploydir=/home/hadoop/current

# The username used to run Azkaban (if applicable)
username=

# The host where the hadoop related scripts are deployed to
deployhost=

# Reference information
refdir=/user/hadoop/data/pp/ref

# Batch urls
run_ps_batch_cmd=http://batch001.phx.ops.pp.bt.ecg.so:8080/batch-api/start/keywordStatsJobRunner?sourceFile=keyword_search_sum.csv&sourceFolder=/tmp
run_results_batch_cmd=http://batch001.phx.ops.pp.bt.ecg.so:8080/batch-api/start/keywordStatsResultJobRunner
run_scrubber_batch_cmd=http://batch001.phx.ops.pp.bt.ecg.so:8080/batch-api/start/keywordStatsScrubberJobRunner
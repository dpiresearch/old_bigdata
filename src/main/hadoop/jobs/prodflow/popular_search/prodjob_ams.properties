# The hdfs directory where logs will be stored
hdfs_ingest_dir=/user/hadoop/data/prod/ingest
#hdfs_ingest_dir=/user/hadoop/data/ingest

# The tomcat directory where logs will be pulled from
# TODO: remove when we remove ingest from the workflow
#tomcat_dir=/var/log/tomcat7multi/frontend
tomcat_dir=/home/hadoop/tmp

# Directory to stage files before hdfs ingestion
# TODO: remove when we remove ingest from the workflow
stg_dir=/home/hadoop/data/stg

# Number of web hosts
# TODO: remove when we remove ingest from the workflow
num_web_hosts=10
web_host_base_name=web

# Input directory for the intermediate jobs
inputdir=/user/hadoop/data/prod/jobs
#inputdir=/user/hadoop/data/ingest/jobs

# The output directory for intermediate and final results
outputdir=/user/hadoop/data/prod/jobs
#outputdir=/user/hadoop/data/ingest/jobs

# The deployment directory
deploydir=/home/hadoop/current

# The username used to run Azkaban (if applicable)
username=

# The host where the hadoop related scripts are deployed to
deployhost=

# Reference information
refdir=/user/hadoop/data/prod/ref

# Batch urls
run_ps_batch_cmd=http://batch.ix5.ops.prod.bt.ecg.so/batch-api/start/popularSearchJobRunnerLite?sourceFile=keyword_search_sum.csv&sourceFolder=/tmp
#run_results_batch_cmd=http://batch.ix5.ops.prod.bt.ecg.so/batch-api/start/keywordStatsResultJobRunner
#run_scrubber_batch_cmd=http://batch.ix5.ops.prod.bt.ecg.so/batch-api/start/keywordStatsScrubberJobRunner

# category and location files
category_file=categories.csv
location_file=za_loc.csv


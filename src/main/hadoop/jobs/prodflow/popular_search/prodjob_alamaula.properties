# The hdfs directory where logs will be stored
hdfs_ingest_dir=/user/hadoop/am/prod/ingest

# The tomcat directory where logs will be pulled from
# TODO: remove when we remove ingest from the workflow
#tomcat_dir=/var/log/tomcat7multi/frontend
tomcat_dir=/home/hadoop/tmp

# Directory to stage files before hdfs ingestion
# TODO: remove when we remove ingest from the workflow
stg_dir=/home/hadoop/alamaula

# Number of web hosts
# TODO: remove when we remove ingest from the workflow
num_web_hosts=20
web_host_base_name=web

# Input directory for the intermediate jobs
inputdir=/user/hadoop/am/prod/jobs

# The output directory for intermediate and final results
outputdir=/user/hadoop/am/prod/jobs

# The deployment directory
deploydir=/media/home/dapang/current/

# The username used to run Azkaban (if applicable)
username=

# The host where the hadoop related scripts are deployed to
deployhost=

# Reference information
refdir=/user/dapang/data/prod/ref

# Batch urls
run_ps_batch_cmd=http://batch.ix5.ops.prod.bt.ecg.so/batch-api/keywordStatsJobRunner?sourceFile=keyword_search_sum.csv&sourceFolder=/tmp
run_results_batch_cmd=http://batch.ix5.ops.prod.bt.ecg.so/batch-api/KeywordStatsResultJobRunner
run_scrubber_batch_cmd=http://batch.ix5.ops.prod.bt.ecg.so/batch-api/start/keywordStatsScrubberJobRunner

